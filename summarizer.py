from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from keyphrasetransformer import KeyPhraseTransformer

kp = KeyPhraseTransformer()

# 1. Load Russian summarization model (RuT5 multitask)
model_name = "cointegrated/rut5-base-multitask"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# 2. Example post text (replace with your content)
# text = """
# üöÄ –°–µ–≥–æ–¥–Ω—è —è —Ä–∞—Å—Å–∫–∞–∂—É, –∫–∞–∫ —É—Å–∫–æ—Ä–∏—Ç—å —Å–±–æ—Ä–∫—É Docker –æ–±—Ä–∞–∑–æ–≤ –≤ CI/CD.
# –ú—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ, multi-stage —Å–±–æ—Ä–∫—É –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é —Å–ª–æ—ë–≤.
# –° –ø—Ä–∏–º–µ—Ä–∞–º–∏ –Ω–∞ GitHub Actions –∏ GitLab.
# –ü–æ–¥—Ä–æ–±–Ω–µ–µ: [—Å—Å—ã–ª–∫–∞]
# """

text = """
–ò–Ω—Ç–µ—Ä–≤—å—é - —ç—Ç–æ –≤—Å–µ–≥–¥–∞ —Å—Ç—Ä–µ—Å—Å –¥–ª—è –∫–∞–Ω–¥–∏–¥–∞—Ç–∞. –ö–æ–Ω–µ—á–Ω–æ: –æ—Ç –µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∑–∞–≤–∏—Å–∏—Ç –±—É–¥—É—â–µ–µ. –ò –Ω–µ –≤—Å–µ–≥–¥–∞ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ —Ö–æ—Ä–æ—à–µ–µ –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏–µ –Ω–∞ –∏–Ω—Ç–µ—Ä–≤—å—é–≤–µ—Ä–∞ –∑–∞ —á–∞—Å —Ä–∞–∑–≥–æ–≤–æ—Ä–∞. –ù–æ —ç—Ç–æ –µ—â—ë –∏ —Å—Ç—Ä–µ—Å—Å –¥–ª—è —Å–∞–º–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤—å—é–≤–µ—Ä–∞, –¥–∞–∂–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–ø—ã—Ç–Ω–æ–≥–æ. –ò–Ω–æ–≥–¥–∞, –∫–æ–≥–¥–∞ –∑–∞–¥–∞—ë—à—å –≤–æ–ø—Ä–æ—Å –∫–∞–Ω–¥–∏–¥–∞—Ç—É, –∞ –æ—Ç–≤–µ—Ç –∑–≤—É—á–∏—Ç –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º, –∫–∞–∂–µ—Ç—Å—è, —á—Ç–æ —ç—Ç–æ —è —á–µ–≥–æ-—Ç–æ –Ω–µ –ø–æ–Ω–∏–º–∞—é, –∞ –Ω–µ —Ç–æ, —á—Ç–æ –∫–∞–Ω–¥–∏–¥–∞—Ç –ø—Ä–æ—Å—Ç–æ –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å, –∏–ª–∏ –Ω–µ –ø–æ–Ω—è–ª –µ–≥–æ. –ù–∞–ø—Ä–∏–º–µ—Ä, —Å–ø—Ä–∞—à–∏–≤–∞–µ—à—å —á–µ–ª–æ–≤–µ–∫–∞ –ø—Ä–æ –ø—Ä–∏–º–µ—Ä —Å–∏—Ç—É–∞—Ü–∏–∏ –∏–∑ –æ–ø—ã—Ç–∞, –∞ –æ–Ω –Ω–∞—á–∏–Ω–∞–µ—Ç –æ–ø–∏—Å—ã–≤–∞—Ç—å –≥–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫—É—é —Å–∏—Ç—É–∞—Ü–∏—é. –ò–ª–∏ —Å–ø—Ä–∞—à–∏–≤–∞–µ—à—å, –∫–∞–∫–æ–π —É—Ä–æ–∫ —á–µ–ª–æ–≤–µ–∫ –≤—ã–Ω–µ—Å –∏–∑ –¥–∞–Ω–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–∏ - –∞ –≤ –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–∞–µ—à—å —Å–ø–∏—Å–æ–∫ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –æ–Ω –ø–æ—Ä–∞–±–æ—Ç–∞–ª. –ü–æ—Å–ª–µ —Ç–∞–∫–æ–≥–æ, –∞–Ω–∞–ª–∏–∑–∏—Ä—É—è –∏–Ω—Ç–µ—Ä–≤—å—é, –≤ —Å–ª–µ–¥—É—é—â–∏–π —Ä–∞–∑ —Å—Ç–∞—Ä–∞–µ—à—å—Å—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã –±–æ–ª–µ–µ —á—ë—Ç–∫–æ, –Ω–æ —ç—Ç–æ –Ω–µ –≤—Å–µ–≥–¥–∞ –ø–æ–º–æ–≥–∞–µ—Ç - –ø–æ—Ç–æ–º—É, —á—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞ –¥–∞–ª–µ–∫–æ –Ω–µ –≤—Å–µ–≥–¥–∞ –≤ –≤–æ–ø—Ä–æ—Å–µ. –ò–Ω–æ–≥–¥–∞ –ø—Ä–æ—Å—Ç–æ –Ω–∞–¥–æ –ø—Ä–∏–Ω—è—Ç—å —Ñ–∞–∫—Ç, —á—Ç–æ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã —Ç–æ–∂–µ –ª—é–¥–∏ –∏ –º–æ–≥—É—Ç –Ω–µ –ø–æ–Ω—è—Ç—å –≤–æ–ø—Ä–æ—Å –∏ –ø–æ—Å—Ç–µ—Å–Ω—è—Ç—å—Å—è —É—Ç–æ—á–Ω–∏—Ç—å, –∏–ª–∏ —Å—Ä–∞–∑—É –Ω–∞—á–∞—Ç—å –æ—Ç–≤–µ—á–∞—Ç—å –∑–∞–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–µ–π, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ —Å–æ–≤—Å–µ–º —Å–æ–æ—Ç–Ω–æ—Å–∏—Ç—Å—è —Å –∑–∞–¥–∞–Ω–Ω—ã–º –≤–æ–ø—Ä–æ—Å–æ–º. –ü–æ–≥–æ–≤–æ—Ä–∏–ª —Å –∫–æ–ª–ª–µ–≥–∞–º–∏ - —É –Ω–∏—Ö —Ç–∞–∫–æ–µ —Ç–æ–∂–µ –±—ã–≤–∞–µ—Ç. –ê —É –≤–∞—Å –±—ã–≤–∞—é—Ç —Ç–∞–∫–∏–µ –º—ã—Å–ª–∏? –ö–∞–∫ —Å–ø—Ä–∞–≤–ª—è–µ—Ç–µ—Å—å?
"""

# 3. Preprocess: remove markdown-like links (simple regex) and extra spaces
import re
clean_text = re.sub(r'\[.*?\]\(.*?\)', '', text)
clean_text = re.sub(r'\s+', ' ', clean_text).strip()

# 4. Prepare input for RuT5 (it expects a task prefix)
# input_text = f"—Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å: {clean_text}"
input_text = f"headline: {clean_text}"

# 5. Tokenize and generate title
inputs = tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True)
summary_ids = model.generate(
    **inputs,
    max_new_tokens=50,     # length of the title
    num_beams=4,           # beam search for quality
    no_repeat_ngram_size=2 # avoid repeating phrases
)

title = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
print("Generated title:", title)

######

# Clean markdown links
# text_clean = re.sub(r'\[.*?\]\(.*?\)', '', text).strip()
#
# input_text = f"keywords: {text_clean}"
#
# inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=512)
#
# summary_ids = model.generate(
#     **inputs,
#     max_new_tokens=40,
#     num_beams=4,
#     no_repeat_ngram_size=2
# )
#
# keywords = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
# print("Extracted tags:", keywords)

# import nltk
# nltk.download('punkt_tab')
#
# tags = kp.get_key_phrases(clean_text)
# print("Tags: ", tags)


from keybert import KeyBERT
from sentence_transformers import SentenceTransformer

# Russian embedding model (high quality)
# model = SentenceTransformer('cointegrated/rubert-tiny2')
# model = SentenceTransformer('cointegrated/rubert-tiny2')
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
kw_model = KeyBERT(model)

# text = """
# –°–µ–≥–æ–¥–Ω—è —Ä–∞–∑–±–∏—Ä–∞–µ–º—Å—è, –∫–∞–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ –ø—Ä–æ–¥–µ.
# –ì–æ–≤–æ—Ä–∏–º –ø—Ä–æ middlewares, –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ, –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ä–∞–±–æ—Ç—É —Å asyncio.
# """

text = clean_text

keywords = kw_model.extract_keywords(
    text,
    # keyphrase_ngram_range=(1, 2),
    keyphrase_ngram_range=(1, 1),
    # stop_words='russian',

    use_mmr=True,
    diversity=0.7,

    top_n=10
)

print(keywords)
